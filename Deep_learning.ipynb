{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanyuSun/Medical-Image-Processing/blob/main/Deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3G6vGd9sChF",
        "outputId": "134725ac-deab-4f39-90ac-7fa64a3a8438"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RyCR9nZ4sYl7",
        "outputId": "4f5150fc-cdb2-41d2-fb58-9d6f9b13ec2b"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmPG9_mqsc2j",
        "outputId": "93683b0d-4eb0-4714-ee46-74a8f9280029"
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 21 12:25:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    32W / 250W |    347MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSc685IarKR5",
        "scrolled": true
      },
      "source": [
        "# example of defining a u-net encoder-decoder generator model\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model\n",
        "from keras.models import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from numpy import vstack\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from numpy import savez_compressed\n",
        "from numpy import load\n",
        "from random import random\n",
        "from numpy.random import randint\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "from numpy import ones\n",
        "from numpy import zeros\n",
        "import os\n",
        "from os import listdir\n",
        "import math\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        " \n",
        "\tfolder = os.path.exists(path)\n",
        " \n",
        "\tif not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
        "\t\tos.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
        "\t\tprint ('new folder')\n",
        "\t\tprint ('OK')\n",
        "\telse:\n",
        "\t\tprint ('There is this folder!')\n",
        "        \n",
        "# def psnr(img1, img2):\n",
        "# \tmse = np.mean((img1 - img2) ** 2 )\n",
        "# \treturn 10 * np.log10(255.0**2/mse)\n",
        "\n",
        "def psnr(img1, img2):\n",
        "  Diff = np.double(img1) - np.double(img2)\n",
        "  mse = np.sum(Diff**2)\n",
        "  return 10 * np.log10(255.0**2/mse)\n",
        "\n",
        "def mae(img1, img2):\n",
        "    mae = np.mean( abs(img1 - img2)  )\n",
        "    return mae \n",
        "\n",
        "\n",
        "# define an encoder block\n",
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# add downsampling layer\n",
        "\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "\t# conditionally add batch normalization\n",
        "\tif batchnorm:\n",
        "\t\tg = BatchNormalization()(g, training=True)\n",
        "\t# leaky relu activation\n",
        "\tg = LeakyReLU(alpha=0.2)(g)\n",
        "\treturn g\n",
        "\n",
        "# define a decoder block\n",
        "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# add upsampling layer\n",
        "\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "\t# add batch normalization\n",
        "\tg = BatchNormalization()(g, training=True)\n",
        "\t# conditionally add dropout\n",
        "\tif dropout:\n",
        "\t\tg = Dropout(0.5)(g, training=True)\n",
        "\t# merge with skip connection\n",
        "\tg = Concatenate()([g, skip_in])\n",
        "\t# relu activation\n",
        "\tg = Activation('relu')(g)\n",
        "\treturn g\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(image_shape=(256,256,1)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# image input\n",
        "\tin_image = Input(shape=image_shape)\n",
        "\t# encoder model: C64-C128-C256-C512-C512-C512-C512-C512\n",
        "\te1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
        "\te2 = define_encoder_block(e1, 128)\n",
        "\te3 = define_encoder_block(e2, 256)\n",
        "\te4 = define_encoder_block(e3, 512)\n",
        "\te5 = define_encoder_block(e4, 512)\n",
        "\te6 = define_encoder_block(e5, 512)\n",
        "\te7 = define_encoder_block(e6, 512)\n",
        "\t# bottleneck, no batch norm and relu\n",
        "\tb = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
        "\tb = Activation('relu')(b)\n",
        "\t# decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
        "\td1 = decoder_block(b, e7, 512)\n",
        "\td2 = decoder_block(d1, e6, 512)\n",
        "\td3 = decoder_block(d2, e5, 512)\n",
        "\td4 = decoder_block(d3, e4, 512, dropout=False)\n",
        "\td5 = decoder_block(d4, e3, 256, dropout=False)\n",
        "\td6 = decoder_block(d5, e2, 128, dropout=False)\n",
        "\td7 = decoder_block(d6, e1, 64, dropout=False)\n",
        "\t# output\n",
        "\tg = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
        "\tout_image = Activation('tanh')(g)\n",
        "\t# define model\n",
        "\tmodel = Model(in_image, out_image)\n",
        "    # compile model\n",
        "\t#opt = Adam(lr=0.0002)\n",
        "\t# model.compile(loss='mae', optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# define the discriminator model\n",
        "def define_discriminator(image_shape):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# source image input\n",
        "\tin_src_image = Input(shape=image_shape)\n",
        "\t# target image input\n",
        "\tin_target_image = Input(shape=image_shape)\n",
        "\t# concatenate images channel-wise\n",
        "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
        "\t# C64\n",
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C128\n",
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C256\n",
        "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C512\n",
        "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# second last output layer\n",
        "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# patch output\n",
        "\tpatch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "# \tpatch_out = Activation('sigmoid')(patch_out)\n",
        "\t# define model\n",
        "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "\treturn model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulf13VS1rKR_"
      },
      "source": [
        "def U_NET(g_model, image_shape):\n",
        "\tg_model.trainable = True\n",
        "\tin1 = Input(shape=image_shape)\n",
        "\tgen_out1 = g_model(in1)\n",
        "\tmodel = Model(in1, gen_out1)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.002)\n",
        "\tmodel.compile(loss='mae', optimizer=opt, loss_weights=1)\n",
        "\treturn model\n",
        "\n",
        "def M_U_NET(g_model1, g_model2, image_shape):\n",
        "\tg_model1.trainable = True\n",
        "\tg_model2.trainable = True\n",
        "\tin1 = Input(shape=image_shape)\n",
        "\tin2 = Input(shape=image_shape)\n",
        "\tgen_out1 = g_model1(in1)\n",
        "\tgen_out2 = g_model2(in2)\n",
        "\tmodel = Model([in1, in2], [gen_out1, gen_out2])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.002)\n",
        "\tmodel.compile(loss=['mae', 'mae'], optimizer=opt, loss_weights=[1,1])\n",
        "\treturn model\n",
        "\n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_PIX2PIX(g_model, d_model, image_shape):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\td_model.trainable = False\n",
        "\t# define the source image\n",
        "\tin_src = Input(shape=image_shape)\n",
        "\t# connect the source image to the generator input\n",
        "\tgen_out = g_model(in_src)\n",
        "\t# connect the source input and generator output to the discriminator input\n",
        "\tdis_out = d_model([in_src, gen_out])\n",
        "\t# src image as input, generated image and classification output\n",
        "\tmodel = Model(in_src, [dis_out, gen_out])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.002, beta_1=0.5)\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[0.01,1])\n",
        "\treturn model\n",
        "\n",
        "def define_M_PIX2PIX(g_model1, g_model2, d_model1, d_model2, image_shape):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\td_model1.trainable = False\n",
        "\td_model2.trainable = False\n",
        "\tg_model1.trainable = True\n",
        "\tg_model2.trainable = True\n",
        "\tin1 = Input(shape=image_shape)\n",
        "\tin2 = Input(shape=image_shape)\n",
        "\tgen_out1 = g_model1(in1)\n",
        "\tgen_out2 = g_model2(in2)\n",
        "\t# connect the source input and generator output to the discriminator input\n",
        "\tdis_out1 = d_model1([in1, gen_out1])\n",
        "\tdis_out2 = d_model2([in1, gen_out1])\n",
        "\tmodel = Model([in1, in2], [gen_out1, gen_out2, dis_out1, dis_out2])\n",
        "\t# compile mode\n",
        "\topt = Adam(lr=0.002, beta_1=0.5)\n",
        "\tmodel.compile(loss=['mae', 'mae', 'binary_crossentropy', 'binary_crossentropy'], optimizer=opt, loss_weights=[1, 1, 0.01, 0.01])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzpWZFC1rKSB",
        "scrolled": true
      },
      "source": [
        "def bytescale(data, cmin=None, cmax=None, high=255, low=0):\n",
        "    \"\"\"\n",
        "    Byte scales an array (image).\n",
        "    Byte scaling means converting the input image to uint8 dtype and scaling\n",
        "    the range to ``(low, high)`` (default 0-255).\n",
        "    If the input image already has dtype uint8, no scaling is done.\n",
        "    This function is only available if Python Imaging Library (PIL) is installed.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        PIL image data array.\n",
        "    cmin : scalar, optional\n",
        "        Bias scaling of small values. Default is ``data.min()``.\n",
        "    cmax : scalar, optional\n",
        "        Bias scaling of large values. Default is ``data.max()``.\n",
        "    high : scalar, optional\n",
        "        Scale max value to `high`.  Default is 255.\n",
        "    low : scalar, optional\n",
        "        Scale min value to `low`.  Default is 0.\n",
        "    Returns\n",
        "    -------\n",
        "    img_array : uint8 ndarray\n",
        "        The byte-scaled array.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from scipy.misc import bytescale\n",
        "    >>> img = np.array([[ 91.06794177,   3.39058326,  84.4221549 ],\n",
        "    ...                 [ 73.88003259,  80.91433048,   4.88878881],\n",
        "    ...                 [ 51.53875334,  34.45808177,  27.5873488 ]])\n",
        "    >>> bytescale(img)\n",
        "    array([[255,   0, 236],\n",
        "           [205, 225,   4],\n",
        "           [140,  90,  70]], dtype=uint8)\n",
        "    >>> bytescale(img, high=200, low=100)\n",
        "    array([[200, 100, 192],\n",
        "           [180, 188, 102],\n",
        "           [155, 135, 128]], dtype=uint8)\n",
        "    >>> bytescale(img, cmin=0, cmax=255)\n",
        "    array([[91,  3, 84],\n",
        "           [74, 81,  5],\n",
        "           [52, 34, 28]], dtype=uint8)\n",
        "    \"\"\"\n",
        "    if data.dtype == np.uint8:\n",
        "        return data\n",
        "\n",
        "    if high > 255:\n",
        "        raise ValueError(\"`high` should be less than or equal to 255.\")\n",
        "    if low < 0:\n",
        "        raise ValueError(\"`low` should be greater than or equal to 0.\")\n",
        "    if high < low:\n",
        "        raise ValueError(\"`high` should be greater than or equal to `low`.\")\n",
        "\n",
        "    if cmin is None:\n",
        "        cmin = data.min()\n",
        "    if cmax is None:\n",
        "        cmax = data.max()\n",
        "\n",
        "    cscale = cmax - cmin\n",
        "    if cscale < 0:\n",
        "        raise ValueError(\"`cmax` should be larger than `cmin`.\")\n",
        "    elif cscale == 0:\n",
        "        cscale = 1\n",
        "\n",
        "    scale = float(high - low) / cscale\n",
        "    bytedata = (data - cmin) * scale + low\n",
        "    return (bytedata.clip(low, high) + 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "# load all images in a directory into memory\n",
        "def load_images(path, size=(256,256)):\n",
        "\tdata_list = list()\n",
        "\t# enumerate filenames in directory, assume all are images\n",
        "\tfor filename in listdir(path):\n",
        "\t\t# load and resize the image\n",
        "\t\tpixels = load_img(path + filename, target_size=size, color_mode='grayscale')\n",
        "\t\t# convert to numpy array\n",
        "\t\tpixels = img_to_array(pixels)\n",
        "\t\t# store\n",
        "\t\tdata_list.append(pixels)\n",
        "\treturn asarray(data_list)\n",
        "\n",
        "# load and prepare training images\n",
        "def load_real_samples(filename):\n",
        "\t# load the dataset\n",
        "\tdata = load(filename)\n",
        "\t# unpack arrays\n",
        "\tX1, X2, X3 = data['arr_0'], data['arr_1'], data['arr_2']\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX1 = (X1 - 127.5) / 127.5\n",
        "\tX2 = (X2 - 127.5) / 127.5\n",
        "\tX3 = (X3 - 127.5) / 127.5\n",
        "\treturn [X1, X2, X3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YM6RH8HrKSC",
        "scrolled": true
      },
      "source": [
        "# select a batch of random samples, returns images and target\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "\t# unpack dataset\n",
        "\ttrainA1,trainA2, trainB = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, trainA1.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX1, X2, Y = trainA1[ix], trainA2[ix], trainB[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "\treturn [X1, X2, Y], y\n",
        "\n",
        "def generate_val_samples(dataset, n_samples):\n",
        "\t# unpack dataset\n",
        "\tvalA, valB, real = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, valA.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX1, X2, Y = valA[ix], valB[ix], real[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\treturn [X1, X2, Y]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUgAx_3IrKSD",
        "scrolled": true
      },
      "source": [
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_sample1(g_model1, sample1, patch_shape):\n",
        "\t# generate fake instance\n",
        "\tY1 = g_model1.predict(sample1)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((len(Y1), patch_shape, patch_shape, 1))\n",
        "\treturn Y1, y\n",
        "\n",
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_sample2(g_model1, g_model2, sample1, sample2, patch_shape):\n",
        "\t# generate fake instance\n",
        "\tY1 = g_model1.predict(sample1)\n",
        "\tY2 = g_model2.predict(sample2)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((len(Y1), patch_shape, patch_shape, 1))\n",
        "\treturn [Y1, Y2], y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogmDA9izrKSD"
      },
      "source": [
        "def summarize_performance1(step, X_in, Y_in, X_out1, path):\n",
        "\n",
        "\n",
        "\t# save plot to file\n",
        "\tfilename1 = '%06d_image_plot.png' % (step+1)\n",
        "\tX = bytescale(X_in[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile1 = path + filename1\n",
        "\timg.save(file1)\n",
        "\n",
        "\n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename2 = '%06d_generate1_plot.png' % (step+1)\n",
        "\tX = bytescale(X_out1[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile1 = path + filename2\n",
        "\timg.save(file1)\n",
        "\n",
        "\n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename3 = '%06d_label_plot.png' % (step+1)\n",
        "\tX = bytescale(Y_in[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile1 = path + filename3\n",
        "\timg.save(file1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glTeLuKdrKSE",
        "scrolled": true
      },
      "source": [
        "def summarize_performance2(step, X_in, Y_in, X_out1, X_out2, path):\n",
        "\n",
        "\n",
        "\t# save plot to file\n",
        "\tfilename1 = '%06d_image_plot.png' % (step+1)\n",
        "\tX = bytescale(X_in[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile1 = path + filename1\n",
        "\timg.save(file1)\n",
        "\n",
        "\n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename2 = '%06d_generate1_plot.png' % (step+1)\n",
        "\tX = bytescale(X_out1[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile1 = path + filename2\n",
        "\timg.save(file1)\n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename4 = '%06d_generate2_plot.png' % (step+1)\n",
        "\tX = bytescale(X_out2[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')    \n",
        "\tfile1 = path + filename4\n",
        "\timg.save(file1)\n",
        "\n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename3 = '%06d_label_plot.png' % (step+1)\n",
        "\tX = bytescale(Y_in[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile1 = path + filename3\n",
        "\timg.save(file1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFjUUGOrrKSF",
        "scrolled": true
      },
      "source": [
        "# save the generator models to file\n",
        "def save_model1(step, g_model_AtoB1, path):\n",
        "\t# save the first generator model\n",
        "\tfilename1 = '%06d_g_model_AtoB1.h5' % (step+1)\n",
        "\tg_model_AtoB1.save(path+filename1)\n",
        "\tprint('>Saved: %s' % (filename1))\n",
        "# save the generator models to file\n",
        "def save_model2(step, g_model_AtoB1, g_model_AtoB2, path):\n",
        "\t# save the first generator model\n",
        "\tfilename1 = '%06d_g_model_AtoB1.h5' % (step+1)\n",
        "\tfilename2 = '%06d_g_model_AtoB2.h5' % (step+1)\n",
        "\tg_model_AtoB1.save(path+filename1)\n",
        "\tg_model_AtoB2.save(path+filename2)\n",
        "\tprint('>Saved: %s,%s' % (filename1,filename2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er9Kzb3rUju8"
      },
      "source": [
        "def generate_real(dataset, ix):\n",
        "\t# unpack dataset\n",
        "\ttrainA1, trainA2, trainB = dataset\n",
        "\t# retrieve selected images\n",
        "\tX1, X2, Y = trainA1[[ix]], trainA2[[ix]], trainB[[ix]]\n",
        "\t# generate 'real' class labels (1)\n",
        "\treturn [X1, X2, Y]\n",
        "\n",
        "def test_all(valset, M_model1, M_model2):\n",
        "\ttestA1, testA2, testB = valset\n",
        "\tts = testA1.shape\n",
        "\tn_steps = ts[0]\n",
        "\tp0 = zeros(n_steps)\n",
        "\tp1 = zeros(n_steps)\n",
        "\tp2 = zeros(n_steps)\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# select a batch of real samples\n",
        "\t\t[X_realA1, X_realA2, X_realB] = generate_real(valset, i)\n",
        "\t\t# generate a batch of fake samples\n",
        "\t\tX_fakeBM1 = M_model1.predict(X_realA1)\n",
        "\t\tX_fakeBM2 = M_model2.predict(X_realA2)\n",
        "\t\tp0[i] = psnr(X_realA1, X_realB)\n",
        "\t\tp1[i] = psnr(X_fakeBM1, X_realB)\n",
        "\t\tp2[i] = psnr(X_fakeBM2, X_realB)\n",
        "\tmp0 = np.mean(p0)\n",
        "\tsp0 = np.var(p0)\n",
        "\tmp1 = np.mean(p1)\n",
        "\tsp1 = np.var(p1)\n",
        "\tmp2 = np.mean(p2)\n",
        "\tsp2 = np.var(p2)\n",
        "\treturn mp0,sp0,mp1,sp1,mp2,sp2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho3pW3dzrKSF",
        "scrolled": true
      },
      "source": [
        "# train pix2pix models\n",
        "def train(u_modelLP, u_model1, u_modelMR, u_model2, m_model1, m_model2, m_model, p_model, p_model1, d_model, M_moel, M_model1, M_model2, d_model1, d_model2, trainset, valset, path_u1, path_u2, path_m, path_p, path_M, n_epochs=50, n_batch=1):\n",
        "\tpath_u1v = path_u1+'val/'\n",
        "\tpath_u2v = path_u2+'val/'\n",
        "\tpath_mv = path_m+'val/'\n",
        "\tpath_pv = path_p+'val/'\n",
        "\tpath_Mv = path_M+'val/'\n",
        "\tmkdir(path_u1v)\n",
        "\tmkdir(path_u2v)\n",
        "\tmkdir(path_mv)\n",
        "\tmkdir(path_pv)\n",
        "\tmkdir(path_Mv)\n",
        "\tmp_old_u = 0\n",
        "\tmp_old_um = 0\n",
        "\tmp_old_mu = 0\n",
        "\tmp_old_p = 0\n",
        "\tmp_old_mp1 = 0\n",
        "\tmp_old_mp2 = 0\n",
        "    # unpack dataset\n",
        "\ttrainA1, trainA2, trainB = trainset\n",
        "\tvalA1, valA2, valB = valset\n",
        "\t# calculate the number of batches per training epoch\n",
        "\tn_patch = d_model.output_shape[1]\n",
        "\tbat_per_epo = int(len(trainA1) / n_batch)\n",
        "\t# calculate the number of training iterations\n",
        "\tn_steps = bat_per_epo * n_epochs\n",
        "\tprint('epo: %d' % (n_steps))\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# select a batch of real samples\n",
        "\t\t[X_realA, X_realB, Y_realA], y_real = generate_real_samples(trainset, n_batch, n_patch)\n",
        "\t\t[X_realAv, X_realBv, Y_realAv] = generate_val_samples(valset, n_batch)\n",
        "\t\t# generate a batch of fake samples\n",
        "\t\tX_fakeAu1, _ = generate_fake_sample1(u_model1, X_realA, n_patch)\n",
        "\t\tX_fakeAu2, _ = generate_fake_sample1(u_model2, X_realB, n_patch)\n",
        "\t\tX_fakeAp, y_fakep = generate_fake_sample1(p_model1, X_realA, n_patch)\n",
        "\t\t[X_fakeAm,X_fakeBm], _ = generate_fake_sample2(m_model1, m_model2, X_realA, X_realB, n_patch)\n",
        "\t\t[X_fakeAM,X_fakeBM], y_fakeM = generate_fake_sample2(M_model1, M_model2, X_realA, X_realB, n_patch)\n",
        "\n",
        "\n",
        "\t\t# generate a batch of val samples\n",
        "\t\tX_fakeAuv1, _ = generate_fake_sample1(u_model1, X_realAv, n_patch)\n",
        "\t\tX_fakeAuv2, _ = generate_fake_sample1(u_model2, X_realBv, n_patch)\n",
        "\t\tX_fakeApv, _ = generate_fake_sample1(p_model1, X_realAv, n_patch)\n",
        "\t\t[X_fakeAmv,X_fakeBmv], _ = generate_fake_sample2(m_model1, m_model2, X_realAv, X_realBv, n_patch)\n",
        "\t\t[X_fakeAMv,X_fakeBMv], _ = generate_fake_sample2(M_model1, M_model2, X_realAv, X_realBv, n_patch)\n",
        "    \n",
        "        \n",
        "\t\tpsnr_t0 = psnr(X_realA[0], Y_realA[0])    \n",
        "\t\tpsnr_tu1 = psnr(X_fakeAu1[0], Y_realA[0])\n",
        "\t\tpsnr_tu2 = psnr(X_fakeAu2[0], Y_realA[0]) \n",
        "\t\tpsnr_tm1 = psnr(X_fakeAm[0], Y_realA[0])\n",
        "\t\tpsnr_tm2 = psnr(X_fakeBm[0], Y_realA[0])\n",
        "\t\tpsnr_tp = psnr(X_fakeAp[0], Y_realA[0])\n",
        "\t\tpsnr_tM1 = psnr(X_fakeAM[0], Y_realA[0])\n",
        "\t\tpsnr_tM2 = psnr(X_fakeBM[0], Y_realA[0])\n",
        "        \n",
        "\t\tpsnr_v0 = psnr(X_realAv[0], Y_realAv[0])    \n",
        "\t\tpsnr_vu1 = psnr(X_fakeAuv1[0], Y_realAv[0]) \n",
        "\t\tpsnr_vu2 = psnr(X_fakeAuv2[0], Y_realAv[0])\n",
        "\t\tpsnr_vm1 = psnr(X_fakeAmv[0], Y_realAv[0])\n",
        "\t\tpsnr_vm2 = psnr(X_fakeBmv[0], Y_realAv[0])\n",
        "\t\tpsnr_vp = psnr(X_fakeApv[0], Y_realAv[0])\n",
        "\t\tpsnr_vM1 = psnr(X_fakeAMv[0], Y_realAv[0])\n",
        "\t\tpsnr_vM2 = psnr(X_fakeBMv[0], Y_realAv[0])\n",
        "\n",
        "\t\tresult2txtt1=str(psnr_t0)\n",
        "\t\tfile_handlet1.write(result2txtt1)\n",
        "\t\tfile_handlet1.write('\\n')\n",
        "        \n",
        "\t\tresult2txtt2=str(psnr_tu1)\n",
        "\t\tfile_handlet2.write(result2txtt2)\n",
        "\t\tfile_handlet2.write('\\n')\n",
        " \n",
        "\t\tresult2txtt3=str(psnr_tm1)\n",
        "\t\tfile_handlet3.write(result2txtt3)\n",
        "\t\tfile_handlet3.write('\\n')\n",
        "         \n",
        "\t\tresult2txtt4=str(psnr_tm2)\n",
        "\t\tfile_handlet4.write(result2txtt4)\n",
        "\t\tfile_handlet4.write('\\n')\n",
        "\n",
        "\t\tresult2txtt5=str(psnr_tp)\n",
        "\t\tfile_handlet5.write(result2txtt5)\n",
        "\t\tfile_handlet5.write('\\n')\n",
        "  \n",
        "\t\tresult2txtt6=str(psnr_tM1)\n",
        "\t\tfile_handlet6.write(result2txtt6)\n",
        "\t\tfile_handlet6.write('\\n')\n",
        "\n",
        "\t\tresult2txtt7=str(psnr_tM2)\n",
        "\t\tfile_handlet7.write(result2txtt7)\n",
        "\t\tfile_handlet7.write('\\n')\n",
        "        \n",
        " \n",
        "        ###############\n",
        "\n",
        "\t\tresult2txtv1=str(psnr_v0)\n",
        "\t\tfile_handlev1.write(result2txtv1)\n",
        "\t\tfile_handlev1.write('\\n')\n",
        "        \n",
        "\t\tresult2txtv2=str(psnr_vu1)\n",
        "\t\tfile_handlev2.write(result2txtv2)\n",
        "\t\tfile_handlev2.write('\\n')\n",
        " \n",
        "\t\tresult2txtv3=str(psnr_vm1)\n",
        "\t\tfile_handlev3.write(result2txtv3)\n",
        "\t\tfile_handlev3.write('\\n')\n",
        "         \n",
        "\t\tresult2txtv4=str(psnr_vm2)\n",
        "\t\tfile_handlev4.write(result2txtv4)\n",
        "\t\tfile_handlev4.write('\\n')\n",
        "\n",
        "\t\tresult2txtv5=str(psnr_vp)\n",
        "\t\tfile_handlev5.write(result2txtv5)\n",
        "\t\tfile_handlev5.write('\\n')\n",
        "  \n",
        "\t\tresult2txtv6=str(psnr_vM1)\n",
        "\t\tfile_handlev6.write(result2txtv6)\n",
        "\t\tfile_handlev6.write('\\n')\n",
        "\n",
        "\t\tresult2txtv7=str(psnr_vM2)\n",
        "\t\tfile_handlev7.write(result2txtv7)\n",
        "\t\tfile_handlev7.write('\\n')\n",
        "\n",
        "         \n",
        "\t\t# update the U-NET\n",
        "\t\tu_loss1 = u_modelLP.train_on_batch(X_realA, Y_realA)\n",
        "\t\tu_loss2 = u_modelMR.train_on_batch(X_realB, Y_realA)\n",
        " \t\t# update the M-U-NET       \n",
        "\t\tm_loss = m_model.train_on_batch([X_realA, X_realB], [Y_realA, Y_realA])\n",
        "        \n",
        "\t\t# update discriminator for real samples\n",
        "\t\td_loss1 = d_model.train_on_batch([X_realA, Y_realA], y_real)\n",
        "\t\t# update discriminator for generated samples\n",
        "\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeAp], y_fakep)\n",
        "\t\t# update the generator\n",
        "\t\tp_loss = p_model.train_on_batch(X_realA, [y_real, Y_realA])\n",
        "              \n",
        " \t\t# update the M-PIX2PIX         \n",
        "\t\t# update discriminator for real samples\n",
        "\t\td_loss11 = d_model1.train_on_batch([X_realA, Y_realA], y_real)\n",
        "\t\t# update discriminator for generated samples\n",
        "\t\td_loss12 = d_model1.train_on_batch([X_realA, X_fakeAM], y_fakeM)       \n",
        "\t\t# update discriminator for real samples\n",
        "\t\td_loss21 = d_model2.train_on_batch([X_realB, Y_realA], y_real)\n",
        "\t\t# update discriminator for generated samples\n",
        "\t\td_loss22 = d_model2.train_on_batch([X_realB, X_fakeBM], y_fakeM)\n",
        "\t\t# update the generator\n",
        "\t\tM_loss = M_moel.train_on_batch([X_realA, X_realB], [Y_realA, Y_realA, y_real, y_real]) \n",
        "        \n",
        "        \n",
        "\t\t# result2txt1=str(u_loss1)\n",
        "\t\t# file_handle1.write(result2txt1)\n",
        "\t\t# file_handle1.write('\\n')\n",
        "        \n",
        "\t\t# result2txt2=str(m_loss[0])\n",
        "\t\t# file_handle2.write(result2txt2)\n",
        "\t\t# file_handle2.write('\\n')\n",
        "\t\t# result2txt3=str(m_loss[1])\n",
        "\t\t# file_handle3.write(result2txt3)\n",
        "\t\t# file_handle3.write('\\n')\n",
        "\t\t# result2txt12=str(m_loss[2])\n",
        "\t\t# file_handle12.write(result2txt12)\n",
        "\t\t# file_handle12.write('\\n')\n",
        "        \n",
        "        \n",
        "\t\t# result2txt9=str(p_loss[0])\n",
        "\t\t# file_handle9.write(result2txt9)\n",
        "\t\t# file_handle9.write('\\n')\n",
        "\t\t# result2txt10=str(p_loss[1])\n",
        "\t\t# file_handle10.write(result2txt10)\n",
        "\t\t# file_handle10.write('\\n')\n",
        "\t\t# result2txt11=str(p_loss[2])\n",
        "\t\t# file_handle11.write(result2txt11)\n",
        "\t\t# file_handle11.write('\\n')\n",
        "        \n",
        "\t\t# summarize performance\n",
        "\t\tprint('>%d,psnr_v0[%.3f],psnr_vu1[%.3f],psnr_vu2[%.3f],psnr_vm1[%.3f],psnr_vm2[%.3f],psnr_vp[%.3f],psnr_vM1[%.3f],psnr_vM2[%.3f]'% (i+1,psnr_v0,psnr_vu1,psnr_vu2,psnr_vm1,psnr_vm2,psnr_vp,psnr_vM1,psnr_vM2) )        \n",
        "        # evaluate the model performance every so often\n",
        "\t\tif mp_old_mp1<psnr_vM1:\n",
        "\t\t\tsummarize_performance2(i, X_realAv, Y_realAv, X_fakeAMv, X_fakeBMv, path_Mv)\n",
        "\t\t\tmp_old_mp1=psnr_vM1\n",
        "\t\t\tprint('>%d' % (i+1))\n",
        "\t\tif mp_old_u<psnr_vu1:\n",
        "\t\t\tsummarize_performance1(i, X_realAv, Y_realAv, X_fakeAuv1, path_u1v)\n",
        "\t\t\tmp_old_u=psnr_vu1\n",
        "\t\tif mp_old_um<psnr_vu2:\n",
        "\t\t\tsummarize_performance1(i, X_realBv, Y_realAv, X_fakeAuv2, path_u2v)\n",
        "\t\t\tmp_old_um=psnr_vu2\n",
        "\t\tif mp_old_mu<psnr_vm1:\n",
        "\t\t\tsummarize_performance2(i, X_realAv, Y_realAv, X_fakeAmv, X_fakeBmv, path_mv)\n",
        "\t\t\tmp_old_mu=psnr_vm1\n",
        "\t\tif mp_old_p<psnr_vp:\n",
        "\t\t\tsummarize_performance1(i, X_realAv, Y_realAv, X_fakeApv, path_pv)\n",
        "\t\t\tmp_old_p=psnr_vp\n",
        "\t\tif mp_old_mp2<psnr_vM2:\n",
        "\t\t\tsummarize_performance2(i, X_realAv, Y_realAv, X_fakeAMv, X_fakeBMv, path_Mv)\n",
        "\t\t\tmp_old_mp2=psnr_vM2\n",
        "\t\tif (i+1) % (bat_per_epo*1) == 0:\n",
        "\t\t\t# save the models\n",
        "\t\t\t# mp0,sp0,mp1,sp1,mp2,sp2 = test_all(valset, u_model1, u_model2)\n",
        "\t\t\t# aa1 = \">\"+str(i)+\",p0:\"+str(mp0)+\"+\"+str(sp0)+\",p1:\"+str(mp1)+\"+\"+str(sp1)+\",p2:\"+str(mp2)+\"+\"+str(sp2)\n",
        "\t\t\t# file_handleMSU.write(aa1)\n",
        "\t\t\t# file_handleMSU.write('\\n')\n",
        "\t\t\t# mp0,sp0,mp1,sp1,mp2,sp2 = test_all(valset, m_model1, m_model2)\n",
        "\t\t\t# aa2 = \">\"+str(i)+\",p0:\"+str(mp0)+\"+\"+str(sp0)+\",p1:\"+str(mp1)+\"+\"+str(sp1)+\",p2:\"+str(mp2)+\"+\"+str(sp2)\n",
        "\t\t\t# file_handleMSmU.write(aa2)\n",
        "\t\t\t# file_handleMSmU.write('\\n')\n",
        "\t\t\t# mp0,sp0,mp1,sp1,mp2,sp2 = test_all(valset, p_model1, p_model1)\n",
        "\t\t\t# aa3 = \">\"+str(i)+\",p0:\"+str(mp0)+\"+\"+str(sp0)+\",p1:\"+str(mp1)+\"+\"+str(sp1)+\",p2:\"+str(mp2)+\"+\"+str(sp2)\n",
        "\t\t\t# file_handleMSp.write(aa3)\n",
        "\t\t\t# file_handleMSp.write('\\n')\n",
        "\t\t\t# summarize_performance1(i, X_realA, Y_realA, X_fakeAu1, path_u1)\n",
        "\t\t\t# summarize_performance1(i, X_realB, Y_realA, X_fakeAu2, path_u2)\n",
        "\t\t\t# summarize_performance1(i, X_realA, Y_realA, X_fakeAp, path_p)\n",
        "\t\t\t# summarize_performance2(i, X_realA, Y_realA, X_fakeAm, X_fakeBm, path_m)\n",
        "\t\t\tprint('MAX,psnr_vu1[%.3f],psnr_vu2[%.3f],psnr_vm1[%.3f],psnr_vp[%.3f],psnr_vM1[%.3f],psnr_vM2[%.3f]'% (mp_old_u,mp_old_um,mp_old_mu,mp_old_p,mp_old_mp1,mp_old_mp2) )                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCnpkfZ8rKSG",
        "outputId": "8dcbdb31-3b2a-425b-fa91-0e23857c66fa"
      },
      "source": [
        "# load test image data\n",
        "\n",
        "dataset = load_real_samples('/content/drive/MyDrive/forpaper/all/L2H.npz')\n",
        "print(dataset[0].shape,dataset[1].shape,dataset[2].shape)\n",
        "\n",
        "valset = load_real_samples('/content/drive/MyDrive/forpaper/all/vL2H.npz')\n",
        "print(valset[0].shape,valset[1].shape,valset[2].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33, 256, 256, 1) (33, 256, 256, 1) (33, 256, 256, 1)\n",
            "(1, 256, 256, 1) (1, 256, 256, 1) (1, 256, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "1kDJ9SrErKSH",
        "scrolled": true,
        "outputId": "fcd215a6-2ac0-4fc1-f207-2c8e1612aee4"
      },
      "source": [
        "# define image shape\n",
        "image_shape = (256,256,1)\n",
        "\n",
        "u_model1 = define_generator(image_shape)\n",
        "u_modelLP = U_NET(u_model1, image_shape)\n",
        "\n",
        "u_model2 = define_generator(image_shape)\n",
        "u_modelMR = U_NET(u_model2, image_shape)\n",
        "\n",
        "m_model1 = define_generator(image_shape)\n",
        "m_model2 = define_generator(image_shape)\n",
        "m_model = M_U_NET(m_model1, m_model2, image_shape)\n",
        "\n",
        "d_model = define_discriminator(image_shape)\n",
        "p_model1 = define_generator(image_shape)\n",
        "p_model = define_PIX2PIX(p_model1, d_model, image_shape)\n",
        "\n",
        "M_model1 = define_generator(image_shape)\n",
        "M_model2 = define_generator(image_shape)\n",
        "d_model1 = define_discriminator(image_shape)\n",
        "d_model2 = define_discriminator(image_shape)\n",
        "M_moel = define_M_PIX2PIX(M_model1,M_model2, d_model1, d_model2, image_shape)\n",
        "\n",
        "PATH = '/content/drive/MyDrive/forpaper/all/'\n",
        "\n",
        "# file_handleMSU=open(PATH+'psnr_MS_U.txt',mode='w')\n",
        "# file_handleMSmU=open(PATH+'psnr_MS_mU.txt',mode='w')\n",
        "# file_handleMSp=open(PATH+'psnr_MS_p.txt',mode='w')\n",
        "\n",
        "# file_handle1=open(PATH+'u_lossu.txt',mode='w')\n",
        "\n",
        "# file_handle2=open(PATH+'mu_lossall.txt',mode='w')\n",
        "# file_handle3=open(PATH+'mu_loss1.txt',mode='w')\n",
        "# file_handle12=open(PATH+'mu_loss2.txt',mode='w')\n",
        "\n",
        "# file_handle9=open(PATH+'p_lossall.txt',mode='w')\n",
        "# file_handle10=open(PATH+'p_loss.txt',mode='w')\n",
        "# file_handle11=open(PATH+'pd_loss.txt',mode='w')\n",
        "\n",
        "file_handlet1=open(PATH+'psnr_t0.txt',mode='w')\n",
        "file_handlet2=open(PATH+'psnr_tu.txt',mode='w')\n",
        "file_handlet3=open(PATH+'psnr_tm1.txt',mode='w')\n",
        "file_handlet4=open(PATH+'psnr_tm2.txt',mode='w')\n",
        "file_handlet5=open(PATH+'psnr_tp.txt',mode='w')\n",
        "file_handlet6=open(PATH+'psnr_tM1.txt',mode='w')\n",
        "file_handlet7=open(PATH+'psnr_tM2.txt',mode='w')\n",
        "\n",
        "file_handlev1=open(PATH+'psnr_v0.txt',mode='w')\n",
        "file_handlev2=open(PATH+'psnr_vu.txt',mode='w')\n",
        "file_handlev3=open(PATH+'psnr_vm1.txt',mode='w')\n",
        "file_handlev4=open(PATH+'psnr_vm2.txt',mode='w')\n",
        "file_handlev5=open(PATH+'psnr_vp.txt',mode='w')\n",
        "file_handlev6=open(PATH+'psnr_vM1.txt',mode='w')\n",
        "file_handlev7=open(PATH+'psnr_vM2.txt',mode='w')\n",
        "\n",
        "path_u1 = '/content/drive/MyDrive/forpaper/all/UNET/'\n",
        "path_u2 = '/content/drive/MyDrive/forpaper/all/UNETm/'\n",
        "path_m = '/content/drive/MyDrive/forpaper/all/MUNET/'\n",
        "path_p = '/content/drive/MyDrive/forpaper/all/PIX2PIX/'\n",
        "path_M = '/content/drive/MyDrive/forpaper/all/MPIX2PIX/'\n",
        "\n",
        "mkdir(path_u1)\n",
        "mkdir(path_u2)\n",
        "mkdir(path_m)\n",
        "mkdir(path_p)\n",
        "mkdir(path_M)\n",
        "\n",
        "# train model\n",
        "train(u_modelLP, u_model1, u_modelMR, u_model2, m_model1, m_model2, m_model, p_model, p_model1, d_model, M_moel, M_model1, M_model2, d_model1, d_model2,dataset, valset, path_u1,path_u2, path_m, path_p,path_M)\n",
        "\n",
        "# file_handleMSU.close()\n",
        "# file_handleMSmU.close()\n",
        "# file_handleMSp.close()\n",
        "\n",
        "# file_handle1.close()\n",
        "# file_handle2.close()\n",
        "# file_handle3.close()\n",
        "# file_handle9.close()\n",
        "# file_handle10.close()\n",
        "# file_handle11.close()\n",
        "# file_handle12.close()\n",
        "\n",
        "file_handlet1.close()\n",
        "file_handlet2.close()\n",
        "file_handlet3.close()\n",
        "file_handlet4.close()\n",
        "file_handlet5.close()\n",
        "file_handlet6.close()\n",
        "file_handlet7.close()\n",
        "\n",
        "file_handlev1.close()\n",
        "file_handlev2.close()\n",
        "file_handlev3.close()\n",
        "file_handlev4.close()\n",
        "file_handlev5.close()\n",
        "file_handlev6.close()\n",
        "file_handlev7.close()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f7e7a6d05928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mu_model1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mu_modelLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU_NET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'define_generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4h240idrKTF"
      },
      "source": [
        "#  save test result\n",
        "def save_test_result1(step, X_in1, Y_in, X_out1, filepath):\n",
        "\n",
        "\t# save plot to file\n",
        "\tfilename1 = '%06d_1_image_plot.png' % (step+1)\n",
        "\tX = bytescale(X_in1[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile1 = filepath + filename1\n",
        "\timg.save(file1)\n",
        "\n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename2 = '%06d_2_generate1_plot.png' % (step+1)\n",
        "\tX = bytescale(X_out1[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile2 = filepath + filename2    \n",
        "\timg.save(file2)\n",
        " \n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename3 = '%06d_4_label_plot.png' % (step+1)\n",
        "\tX = bytescale(Y_in[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile3 = filepath + filename3\n",
        "\timg.save(file3)\n",
        "\n",
        "\n",
        "def save_test_result2(step, X_in1, Y_in, X_out1, X_out2, filepath):\n",
        "\n",
        "\t# save plot to file\n",
        "\tfilename1 = '%06d_1_image_plot.png' % (step+1)\n",
        "\tX = bytescale(X_in1[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile1 = filepath + filename1\n",
        "\timg.save(file1)\n",
        "\n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename2 = '%06d_2_generate1_plot.png' % (step+1)\n",
        "\tX = bytescale(X_out1[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile2 = filepath + filename2    \n",
        "\timg.save(file2)\n",
        "    \n",
        " \t# save plot to file\n",
        "\tfilename3 = '%06d_3_generate2_plot.png' % (step+1)\n",
        "\tX = bytescale(X_out2[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile3 = filepath + filename3    \n",
        "\timg.save(file3)   \n",
        "\n",
        "    \n",
        "\t# save plot to file\n",
        "\tfilename3 = '%06d_4_label_plot.png' % (step+1)\n",
        "\tX = bytescale(Y_in[0], cmin=-1, cmax=1, high=255, low=0)\n",
        "\tX = np.squeeze(X)\n",
        "\timg = Image.fromarray(X, 'L')\n",
        "\tfile3 = filepath + filename3\n",
        "\timg.save(file3)\n",
        "\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# unpack dataset\n",
        "\ttrainA, trainB = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, trainA.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX1, X2 = trainA[ix], trainB[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\treturn [X1, X2]    \n",
        "    \n",
        "def generate_real(dataset, ix):\n",
        "\t# unpack dataset\n",
        "\ttrainA1, trainA2, trainB = dataset\n",
        "\t# retrieve selected images\n",
        "\tX1, X2, Y = trainA1[[ix]], trainA2[[ix]], trainB[[ix]]\n",
        "\t# generate 'real' class labels (1)\n",
        "\treturn [X1, X2, Y]\n",
        "    \n",
        "def test(u_model1, u_model2, m_model1, m_model2, p_model, dataset, savepath_u1,savepath_u2, savepath_m, savepath_p):\n",
        "\t# unpack dataset\n",
        "\ttestA1, testA2, testB = dataset\n",
        "\tprint(testA1.shape, testA2.shape, testB.shape)\n",
        "\tts = testA1.shape\n",
        "\tn_steps = ts[0]\n",
        "\tprint('num: %d' % (n_steps))\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_steps):\n",
        "\t\t# select a batch of real samples\n",
        "\t\t[X_realA1, X_realA2, X_realB] = generate_real(dataset, i)\n",
        "\t\t# generate a batch of fake samples\n",
        "\t\tX_fakeBu1 = u_model1.predict(X_realA1)\n",
        "\t\tX_fakeBu2 = u_model2.predict(X_realA2)\n",
        "\t\tX_fakeBm1 = m_model1.predict(X_realA1)\n",
        "\t\tX_fakeBm2 = m_model2.predict(X_realA2)\n",
        "\t\tX_fakeBp = p_model.predict(X_realA1)\n",
        "\t\tsave_test_result1(i , X_realA1, X_realB, X_fakeBu1, savepath_u1)\n",
        "\t\tsave_test_result1(i , X_realA2, X_realB, X_fakeBu2, savepath_u2)\n",
        "\t\tsave_test_result1(i , X_realA1, X_realB, X_fakeBp, savepath_p)\n",
        "\t\tsave_test_result2(i , X_realA1, X_realB, X_fakeBm1, X_fakeBm2, savepath_m)\n",
        "\t\tprint('>num: %d done' % (i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVgYAWeErKTF",
        "outputId": "bfe78e0f-5255-404f-be3e-c88810faa61b"
      },
      "source": [
        "# load test image data\n",
        "testset = load_real_samples('/content/drive/MyDrive/forpaper/3/vL2H.npz')\n",
        "print(testset[0].shape,testset[1].shape, testset[2].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(170, 256, 256, 1) (170, 256, 256, 1) (170, 256, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "eJJK9XVurKTG",
        "scrolled": true,
        "outputId": "1a3bad0b-f01b-47bc-8126-b45b934fcb07"
      },
      "source": [
        "# example of using saved cyclegan models for image translation  078375_g_model_AtoB2\n",
        "from keras.models import load_model\n",
        "\n",
        "u_model1 = load_model('/content/drive/MyDrive/forpaper/3/UNET/001632_g_model_AtoB1.h5')\n",
        "u_model2 = load_model('/content/drive/MyDrive/forpaper/3/UNETm/001632_g_model_AtoB1.h5')\n",
        "\n",
        "m_model1 = load_model('/content/drive/MyDrive/forpaper/3/MUNET/001632_g_model_AtoB1.h5')\n",
        "m_model2 = load_model('/content/drive/MyDrive/forpaper/3/MUNET/001632_g_model_AtoB2.h5')\n",
        "\n",
        "p_model = load_model('/content/drive/MyDrive/forpaper/3/PIX2PIX/001632_g_model_AtoB1.h5')\n",
        "\n",
        "path_u1 = '/content/drive/MyDrive/forpaper/3/UNET/val/'\n",
        "path_u2 = '/content/drive/MyDrive/forpaper/3/UNETm/val/'\n",
        "path_m = '/content/drive/MyDrive/forpaper/3/MUNET/val/'\n",
        "path_p = '/content/drive/MyDrive/forpaper/3/PIX2PIX/val/'\n",
        "\n",
        "mkdir(path_u1)\n",
        "mkdir(path_u2)\n",
        "mkdir(path_m)\n",
        "mkdir(path_p)\n",
        "test(u_model1, u_model2, m_model1, m_model2, p_model, testset, path_u1,path_u2, path_m, path_p)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f75e083f2188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mu_model1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/forpaper/3/UNET/001632_g_model_AtoB1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mu_model2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/forpaper/3/UNETm/001632_g_model_AtoB1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 121\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /content/drive/MyDrive/forpaper/3/UNET/001632_g_model_AtoB1.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE6XUFhtrKTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994dd182-2c8f-4c44-ff86-78ff9901ff9f"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy\n",
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from numpy import vstack\n",
        "import shutil\n",
        "\n",
        "def mkdir(path):\n",
        " \n",
        "\tfolder = os.path.exists(path)\n",
        " \n",
        "\tif not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
        "\t\tos.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
        "\t\tprint ('new folder')\n",
        "\t\tprint ('OK')\n",
        "\telse:\n",
        "\t\tprint ('There is this folder!')\n",
        "\n",
        "path_p = 'D:/for_paper/U&MU&P/e2/PIX2PIX/val/'\n",
        "mkdir(path_p+'image')\n",
        "mkdir(path_p+'gen1')\n",
        "mkdir(path_p+'gen2')\n",
        "mkdir(path_p+'label')\n",
        "#D:\\TEST\\pytorch-CycleGAN-and-pix2pix-master\\full_data\\1-pytorch-CycleGAN-100%-640-256-neck-7-1-r - ta\\pytorch-CycleGAN-and-pix2pix-master\\results\\L2H\\test_latest\\images\n",
        "file =os.listdir(path_p)\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"image\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_p+file_,path_p+'image/'+filename)\n",
        "\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"generate1\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_p+file_,path_p+'gen1/'+filename)\n",
        "\n",
        "# for file_ in file:     #循环读取每个文件名\n",
        "#     if file_.find(\"generate2\") > 0:\n",
        "#         filename = file_[0:6] + \".png\"\n",
        "#         print(filename)\n",
        "#         shutil.copy(path_p+file_,path_p+'gen2/'+filename)\n",
        "\n",
        "# for file_ in file:     #循环读取每个文件名\n",
        "#     if file_.find(\"generate3\") > 0:\n",
        "#         filename = file_[0:6] + \".png\"\n",
        "#         print(filename)\n",
        "#         shutil.copy(path+file_,path+'gen3/'+filename)\n",
        "    \n",
        "        \n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"label\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_p+file_,path_p+'label/'+filename)\n",
        "    \n",
        "path_m = 'D:/for_paper/U&MU&P/e2/MUNET/val/'\n",
        "mkdir(path_m+'image')\n",
        "mkdir(path_m+'gen1')\n",
        "mkdir(path_m+'gen2')\n",
        "mkdir(path_m+'label')\n",
        "#D:\\TEST\\pytorch-CycleGAN-and-pix2pix-master\\full_data\\1-pytorch-CycleGAN-100%-640-256-neck-7-1-r - ta\\pytorch-CycleGAN-and-pix2pix-master\\results\\L2H\\test_latest\\images\n",
        "file =os.listdir(path_m)\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"image\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_m+file_,path_m+'image/'+filename)\n",
        "\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"generate1\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_m+file_,path_m+'gen1/'+filename)\n",
        "\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"generate2\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_m+file_,path_m+'gen2/'+filename)\n",
        "\n",
        "# for file_ in file:     #循环读取每个文件名\n",
        "#     if file_.find(\"generate3\") > 0:\n",
        "#         filename = file_[0:6] + \".png\"\n",
        "#         print(filename)\n",
        "#         shutil.copy(path+file_,path+'gen3/'+filename)\n",
        "    \n",
        "        \n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"label\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_m+file_,path_m+'label/'+filename)\n",
        "\n",
        "\n",
        "path_u1 = 'D:/for_paper/U&MU&P/e2/UNET1/val/'\n",
        "mkdir(path_u1+'image')\n",
        "mkdir(path_u1+'gen1')\n",
        "mkdir(path_u1+'gen2')\n",
        "mkdir(path_u1+'label')\n",
        "#D:\\TEST\\pytorch-CycleGAN-and-pix2pix-master\\full_data\\1-pytorch-CycleGAN-100%-640-256-neck-7-1-r - ta\\pytorch-CycleGAN-and-pix2pix-master\\results\\L2H\\test_latest\\images\n",
        "file =os.listdir(path_u1)\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"image\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_u1+file_,path_u1+'image/'+filename)\n",
        "\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"generate1\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_u1+file_,path_u1+'gen1/'+filename)\n",
        "\n",
        "# for file_ in file:     #循环读取每个文件名\n",
        "#     if file_.find(\"generate2\") > 0:\n",
        "#         filename = file_[0:6] + \".png\"\n",
        "#         print(filename)\n",
        "#         shutil.copy(path_p+file_,path_p+'gen2/'+filename)\n",
        "\n",
        "# for file_ in file:     #循环读取每个文件名\n",
        "#     if file_.find(\"generate3\") > 0:\n",
        "#         filename = file_[0:6] + \".png\"\n",
        "#         print(filename)\n",
        "#         shutil.copy(path+file_,path+'gen3/'+filename)\n",
        "    \n",
        "        \n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"label\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_u1+file_,path_u1+'label/'+filename)\n",
        "        \n",
        "        \n",
        "path_u2 = 'D:/for_paper/U&MU&P/e2/UNET2/val/'\n",
        "mkdir(path_u2+'image')\n",
        "mkdir(path_u2+'gen1')\n",
        "mkdir(path_u2+'gen2')\n",
        "mkdir(path_u2+'label')\n",
        "#D:\\TEST\\pytorch-CycleGAN-and-pix2pix-master\\full_data\\1-pytorch-CycleGAN-100%-640-256-neck-7-1-r - ta\\pytorch-CycleGAN-and-pix2pix-master\\results\\L2H\\test_latest\\images\n",
        "file =os.listdir(path_u2)\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"image\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_u2+file_,path_u2+'image/'+filename)\n",
        "\n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"generate1\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_u2+file_,path_u2+'gen1/'+filename)\n",
        "\n",
        "# for file_ in file:     #循环读取每个文件名\n",
        "#     if file_.find(\"generate2\") > 0:\n",
        "#         filename = file_[0:6] + \".png\"\n",
        "#         print(filename)\n",
        "#         shutil.copy(path_p+file_,path_p+'gen2/'+filename)\n",
        "\n",
        "# for file_ in file:     #循环读取每个文件名\n",
        "#     if file_.find(\"generate3\") > 0:\n",
        "#         filename = file_[0:6] + \".png\"\n",
        "#         print(filename)\n",
        "#         shutil.copy(path+file_,path+'gen3/'+filename)\n",
        "    \n",
        "        \n",
        "for file_ in file:     #循环读取每个文件名\n",
        "    if file_.find(\"label\") > 0:\n",
        "        filename = file_[0:6] + \".png\"\n",
        "        print(filename)\n",
        "        shutil.copy(path_u2+file_,path_u2+'label/'+filename)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n",
            "new folder\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYRbrJfOrKTH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}